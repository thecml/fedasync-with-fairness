Async experiments on MNIST/CIFAR10/FEMNIST

Data:
- Partitioned MNIST (LDR)
- Partitioned CIFAR10 (LDR)
- FEMINST

Objective: Mitigate the drawback that larger clients are punished unfairly in FedAsync
Show that training with proposals 1-3 given more stable training in terms of loss than SOTA

Number of clients: 10/100
Max server aggregations: 1000

Runs:

poly(1)
proposal1
proposal2
proposal3

- Avg. staleness over N aggregations (poly vs proposals)
- Train/valid loss over N aggregations (poly vs propoals)
- Train/valid acc over N aggregations (poly vs propoals)
